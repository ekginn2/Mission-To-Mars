{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission to Mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn= \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.collection.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize browser\n",
    "def init_browser():\n",
    "    # @NOTE: Replace the path with your actual path to the chromedriver\n",
    "    executable_path = {\"executable_path\": \"./chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape NASA title\n",
    "def scrape_nasa_title():\n",
    "\n",
    "    # Initialize browser\n",
    "    nasa_browser = init_browser()\n",
    "\n",
    "    # Visit the nasa site\n",
    "    nasa_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "    nasa_browser.visit(nasa_url)\n",
    "\n",
    "    # Scrape page into soup\n",
    "    nasa_html = nasa_browser.html\n",
    "    nasa_soup = BeautifulSoup(nasa_html, \"html.parser\")\n",
    "\n",
    "    # Find title\n",
    "    content_title = nasa_soup.find(\n",
    "        \"div\", class_=\"content_title\"\n",
    "    ).get_text()\n",
    "    return content_title\n",
    "\n",
    "# Function to scrape NASA text\n",
    "def scrape_nasa_text():\n",
    "\n",
    "    # Initialize browser\n",
    "    nasa_browser = init_browser()\n",
    "\n",
    "    # Visit the nasa site\n",
    "    nasa_url = \"https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest\"\n",
    "    nasa_browser.visit(nasa_url)\n",
    "\n",
    "    # Scrape page into soup\n",
    "    nasa_html = nasa_browser.html\n",
    "    nasa_soup = BeautifulSoup(nasa_html, \"html.parser\")\n",
    "\n",
    "\n",
    "    # Find paragraph text\n",
    "    content_text = nasa_soup.find(\n",
    "        \"div\", class_=\"article_teaser_body\"\n",
    "    ).get_text()\n",
    "    return content_text\n",
    "\n",
    "\n",
    "news_title = scrape_nasa_title()\n",
    "news_p = scrape_nasa_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov//spaceimages/images/wallpaper/PIA18049-1920x1200.jpg'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to scrape JPL data\n",
    "def scrape_jpl():\n",
    "\n",
    "    # Initialize browser\n",
    "    jpl_browser = init_browser()\n",
    "\n",
    "    # Visit the nasa jpl site\n",
    "    jpl_url = \"https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars\"\n",
    "    jpl_browser.visit(jpl_url)\n",
    "\n",
    "    # Scrape page into soup\n",
    "    jpl_html = jpl_browser.html\n",
    "    jpl_soup = BeautifulSoup(jpl_html, \"html.parser\")\n",
    "\n",
    "    # Find featured image\n",
    "    mars_image = \"https://www.jpl.nasa.gov/\" + jpl_soup.find(\n",
    "        \"section\", class_=\"centered_text clearfix main_feature primary_media_feature single\"\n",
    "    ).find(\n",
    "    \"article\", class_=\"carousel_item\"\n",
    "    )[\"style\"].split()[1][5:57]\n",
    "    return mars_image\n",
    "\n",
    "\n",
    "mars_imageurl = scrape_jpl()\n",
    "mars_imageurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape weather data\n",
    "def scrape_weather():\n",
    "\n",
    "    # Initialize browser\n",
    "    weather_browser = init_browser()\n",
    "\n",
    "    # Visit the mars twitter site\n",
    "    weather_url = \"https://twitter.com/marswxreport?lang=en\"\n",
    "    weather_browser.visit(weather_url)\n",
    "\n",
    "    # Scrape page into soup\n",
    "    weather_html = weather_browser.html\n",
    "    weather_soup = BeautifulSoup(weather_html, \"html.parser\")\n",
    "\n",
    "    # Find info\n",
    "    mars_tweet = weather_soup.find(\n",
    "        \"p\", class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\"\n",
    "    ).text\n",
    "    mars_tweet\n",
    "\n",
    "    # Return results\n",
    "    return mars_tweet\n",
    "\n",
    "mars_recent_tweet = scrape_weather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to scrape Mars facts\n",
    "def scrape_facts():\n",
    "\n",
    "    # Visit the specefacts site\n",
    "    facts_url = \"https://space-facts.com/mars/\"\n",
    "    \n",
    "    mars_facttable = pd.read_html(facts_url)\n",
    "\n",
    "    # Return results\n",
    "    mars_factsdf = mars_facttable[0]\n",
    "    mars_factsdf.rename( columns = {0 : \"fact\", 1 : \"value\"})\n",
    "    \n",
    "    mars_factshtml = mars_factsdf.to_html\n",
    "    \n",
    "    return(mars_factshtml)\n",
    "\n",
    "mars_facts = scrape_facts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape hemisphere data\n",
    "def scrape_hemispheres():\n",
    "\n",
    "    # Initialize browser\n",
    "    hemis_browser = init_browser()\n",
    "\n",
    "    # Visit the Astrogeology site\n",
    "    hemis_url = \"https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars\"\n",
    "    \n",
    "    hemis_url1 = \"https://astrogeology.usgs.gov/search/map/Mars/Viking/cerberus_enhanced\"\n",
    "    hemis_url2 = \"https://astrogeology.usgs.gov/search/map/Mars/Viking/schiaparelli_enhanced\"\n",
    "    hemis_url3 = \"https://astrogeology.usgs.gov/search/map/Mars/Viking/syrtis_major_enhanced\"\n",
    "    hemis_url4 = \"https://astrogeology.usgs.gov/search/map/Mars/Viking/valles_marineris_enhanced\"\n",
    "    \n",
    "    #URL 1\n",
    "    hemis_browser.visit(hemis_url1)\n",
    "\n",
    "    # Scrape page into soup\n",
    "    hemis_html1 = hemis_browser.html\n",
    "    hemis_soup1 = BeautifulSoup(hemis_html1, \"html.parser\")\n",
    "    \n",
    "    # Find info\n",
    "    hemis_photo1 = hemis_soup1.find(\n",
    "        \"img\", class_=\"wide-image\"\n",
    "    )[\"src\"]\n",
    "    \n",
    "    hemis_title1 = hemis_soup1.find(\n",
    "        \"h2\", class_=\"title\"\n",
    "    ).text\n",
    "    \n",
    "    \n",
    "    #URL 2\n",
    "    hemis_browser.visit(hemis_url2)\n",
    "    \n",
    "    # Scrape page into soup\n",
    "    hemis_html2 = hemis_browser.html\n",
    "    hemis_soup2 = BeautifulSoup(hemis_html2, \"html.parser\")\n",
    "    \n",
    "    # Find info\n",
    "    hemis_photo2 = hemis_soup2.find(\n",
    "        \"img\", class_=\"wide-image\"\n",
    "    )[\"src\"]\n",
    "    \n",
    "    hemis_title2 = hemis_soup2.find(\n",
    "        \"h2\", class_=\"title\"\n",
    "    ).text\n",
    "    \n",
    "    \n",
    "    #URL 3\n",
    "    hemis_browser.visit(hemis_url3)\n",
    "    \n",
    "    # Scrape page into soup\n",
    "    hemis_html3 = hemis_browser.html\n",
    "    hemis_soup3 = BeautifulSoup(hemis_html3, \"html.parser\")\n",
    "    \n",
    "    # Find info\n",
    "    hemis_photo3 = hemis_soup3.find(\n",
    "        \"img\", class_=\"wide-image\"\n",
    "    )[\"src\"]\n",
    "    \n",
    "    hemis_title3 = hemis_soup3.find(\n",
    "        \"h2\", class_=\"title\"\n",
    "    ).text\n",
    "    \n",
    "    \n",
    "    #URL 4\n",
    "    hemis_browser.visit(hemis_url4)\n",
    "    \n",
    "    # Scrape page into soup\n",
    "    hemis_html4 = hemis_browser.html\n",
    "    hemis_soup4 = BeautifulSoup(hemis_html4, \"html.parser\")\n",
    "    \n",
    "    # Find info\n",
    "    hemis_photo4 = hemis_soup4.find(\n",
    "        \"img\", class_=\"wide-image\"\n",
    "    )[\"src\"]\n",
    "    \n",
    "    hemis_title4 = hemis_soup4.find(\n",
    "        \"h2\", class_=\"title\"\n",
    "    ).text\n",
    "\n",
    "    \n",
    "    \n",
    "    # Store in dictionary\n",
    "    hemisphere_image_urls = [\n",
    "    {\"title\": hemis_title1, \"img_url\": hemis_photo1},\n",
    "    {\"title\": hemis_title2, \"img_url\": hemis_photo2},\n",
    "    {\"title\": hemis_title3, \"img_url\": hemis_photo3},\n",
    "    {\"title\": hemis_title4, \"img_url\": hemis_photo4},\n",
    "    ]\n",
    "\n",
    "\n",
    "    # Return results\n",
    "    return hemisphere_image_urls\n",
    "\n",
    "mars_hemispheres = scrape_hemispheres()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_dict = {\n",
    "    \"news_title\" : news_title,\n",
    "    \"news_text\" : news_p,\n",
    "    \"mars_image\" : mars_imageurl,\n",
    "    \"mars_tweet\" : mars_recent_tweet,\n",
    "    \"mars_facts:\" : mars_facts,\n",
    "    \"mars_hemispheres\" : mars_hemispheres\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(type(mars_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov//spaceimages/images/wallpaper/PIA18049-1920x1200.jpg'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_imageurl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': '/cache/images/cfa62af2557222a02478f1fcd781d445_cerberus_enhanced.tif_full.jpg'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': '/cache/images/3cdd1cbf5e0813bba925c9030d13b62e_schiaparelli_enhanced.tif_full.jpg'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': '/cache/images/ae209b4e408bb6c3e67b6af38168cf28_syrtis_major_enhanced.tif_full.jpg'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': '/cache/images/7cf2da4bf549ed01c17f206327be4db7_valles_marineris_enhanced.tif_full.jpg'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_hemispheres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
